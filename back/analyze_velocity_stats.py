#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VelocityHedgeHold ç­–ç•¥é€Ÿåº¦ç»Ÿè®¡åˆ†æå·¥å…·

åŠŸèƒ½ï¼š
1. ä»æ—¥å¿—æˆ–æ•°æ®æ–‡ä»¶ä¸­æå–ä»·æ ¼æ•°æ®
2. è®¡ç®—ä¸åŒçª—å£å¤§å°ä¸‹çš„é€Ÿåº¦
3. ç»Ÿè®¡æ»¡è¶³ä¸åŒé€Ÿåº¦/ä½ç§»æ¡ä»¶çš„æ¦‚ç‡
4. ç”Ÿæˆå‚æ•°é…ç½®å»ºè®®
"""

import re
import json
import csv
import os
import sys
from pathlib import Path
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import List, Tuple, Dict, Optional
import statistics

class PriceSample:
    """ä»·æ ¼æ ·æœ¬"""
    def __init__(self, timestamp: datetime, price_cents: int, token_type: str):
        self.timestamp = timestamp
        self.price_cents = price_cents
        self.token_type = token_type  # 'up' or 'down'

class VelocityCalculator:
    """é€Ÿåº¦è®¡ç®—å™¨ï¼ˆä¸ Go ä»£ç é€»è¾‘ä¸€è‡´ï¼‰"""
    
    def __init__(self, window_seconds: int):
        self.window_seconds = window_seconds
        self.samples_up = deque()
        self.samples_down = deque()
    
    def add_sample(self, sample: PriceSample):
        """æ·»åŠ ä»·æ ¼æ ·æœ¬"""
        if sample.token_type == 'up':
            self.samples_up.append(sample)
        else:
            self.samples_down.append(sample)
        self._prune(sample.timestamp)
    
    def _prune(self, now: datetime):
        """æ¸…ç†è¿‡æœŸæ ·æœ¬ï¼ˆä¸ Go ä»£ç é€»è¾‘ä¸€è‡´ï¼‰"""
        cutoff = now - timedelta(seconds=self.window_seconds)
        
        # æ¸…ç† UP æ ·æœ¬
        while self.samples_up and self.samples_up[0].timestamp < cutoff:
            self.samples_up.popleft()
        
        # æ¸…ç† DOWN æ ·æœ¬
        while self.samples_down and self.samples_down[0].timestamp < cutoff:
            self.samples_down.popleft()
    
    def compute_velocity(self, token_type: str) -> Optional[Dict]:
        """è®¡ç®—é€Ÿåº¦ï¼ˆä¸ Go ä»£ç é€»è¾‘ä¸€è‡´ï¼‰"""
        samples = self.samples_up if token_type == 'up' else self.samples_down
        
        if len(samples) < 2:
            return None
        
        first = samples[0]
        last = samples[-1]
        
        dt = (last.timestamp - first.timestamp).total_seconds()
        if dt <= 0.001:
            return None
        
        delta = last.price_cents - first.price_cents
        if delta <= 0:  # åªè®¡ç®—ä¸Šè¡Œï¼ˆä¸ Go ä»£ç ä¸€è‡´ï¼‰
            return None
        
        velocity = delta / dt
        
        if velocity != velocity or abs(velocity) == float('inf'):  # NaN or Inf
            return None
        
        return {
            'ok': True,
            'delta': delta,
            'seconds': dt,
            'velocity': velocity
        }

class LogParser:
    """æ—¥å¿—è§£æå™¨"""
    
    @staticmethod
    def parse_timestamp(line: str) -> Optional[datetime]:
        """è§£ææ—¥å¿—æ—¶é—´æˆ³"""
        # æ ¼å¼: [25-12-30 16:03:10]
        pattern = r'\[(\d+)-(\d+)-(\d+)\s+(\d+):(\d+):(\d+)\]'
        match = re.search(pattern, line)
        if match:
            year, month, day, hour, minute, second = match.groups()
            try:
                # å‡è®¾æ˜¯ 2025 å¹´
                return datetime(2025, int(month), int(day), int(hour), int(minute), int(second))
            except:
                pass
        return None
    
    @staticmethod
    def extract_price_from_log(line: str) -> Optional[Tuple[str, int]]:
        """ä»æ—¥å¿—ä¸­æå–ä»·æ ¼ä¿¡æ¯"""
        # åŒ¹é…æ ¼å¼: âš¡ [velocityhedgehold] å‡†å¤‡è§¦å‘: side=up entryAsk=92c ...
        pattern = r'side=(up|down)\s+entryAsk=(\d+)c'
        match = re.search(pattern, line)
        if match:
            token_type = match.group(1)
            price_cents = int(match.group(2))
            return (token_type, price_cents)
        
        # åŒ¹é…æ ¼å¼: ğŸ“¥ [sessionPriceHandler] é¦–æ¬¡æ”¶åˆ°ä»·æ ¼äº‹ä»¶: up @ 0.5400
        pattern = r'ä»·æ ¼äº‹ä»¶:\s*(up|down)\s+@\s+([\d.]+)'
        match = re.search(pattern, line)
        if match:
            token_type = match.group(1)
            price_decimal = float(match.group(2))
            price_cents = int(price_decimal * 100 + 0.5)
            return (token_type, price_cents)
        
        # åŒ¹é…æ ¼å¼: ğŸ“¥ [sessionPriceHandler] é¦–æ¬¡æ”¶åˆ°ä»·æ ¼äº‹ä»¶: up @ 0.5400 (Session=polymarket)
        pattern = r'é¦–æ¬¡æ”¶åˆ°ä»·æ ¼äº‹ä»¶:\s*(up|down)\s+@\s+([\d.]+)'
        match = re.search(pattern, line)
        if match:
            token_type = match.group(1)
            price_decimal = float(match.group(2))
            price_cents = int(price_decimal * 100 + 0.5)
            return (token_type, price_cents)
        
        # åŒ¹é…æ ¼å¼: [price_change->price] æˆ–å…¶ä»–ä»·æ ¼æ›´æ–°æ—¥å¿—
        # å°è¯•ä»ç›˜å£ä»·å·®æ—¥å¿—ä¸­æå–: UP: bid=XXc ask=XXc DOWN: bid=XXc ask=XXc
        pattern = r'UP:\s*bid=(\d+)c\s+ask=(\d+)c.*DOWN:\s*bid=(\d+)c\s+ask=(\d+)c'
        match = re.search(pattern, line)
        if match:
            # è¿”å› UP å’Œ DOWN çš„ mid ä»·æ ¼
            up_bid, up_ask, down_bid, down_ask = map(int, match.groups())
            up_mid = (up_bid + up_ask) // 2
            down_mid = (down_bid + down_ask) // 2
            # è¿”å›ä¸¤ä¸ªä»·æ ¼æ ·æœ¬ï¼ˆéœ€è¦è°ƒç”¨è€…å¤„ç†ï¼‰
            return ('up', up_mid)  # å…ˆè¿”å› UPï¼ŒDOWN éœ€è¦å•ç‹¬å¤„ç†
        
        return None

class CSVDataParser:
    """CSV æ•°æ®è§£æå™¨ï¼ˆç”¨äº datarecorder ç”Ÿæˆçš„æ•°æ®ï¼‰"""
    
    @staticmethod
    def parse_csv(file_path: str) -> List[PriceSample]:
        """è§£æ CSV æ–‡ä»¶"""
        samples = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    try:
                        # å°è¯•å¤šç§æ—¶é—´æˆ³æ ¼å¼
                        timestamp_str = row.get('Timestamp', '') or row.get('timestamp', '')
                        if not timestamp_str:
                            continue
                        
                        # å°è¯•è§£ææ—¶é—´æˆ³ï¼ˆå¯èƒ½æ˜¯ Unix æ—¶é—´æˆ³æˆ– ISO æ ¼å¼ï¼‰
                        try:
                            if timestamp_str.isdigit():
                                timestamp = datetime.fromtimestamp(int(timestamp_str))
                            else:
                                timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                        except:
                            continue
                        
                        # å°è¯•å¤šç§ä»·æ ¼å­—æ®µå
                        up_price = float(row.get('UpPrice', 0) or row.get('up_price', 0) or row.get('UP', 0))
                        down_price = float(row.get('DownPrice', 0) or row.get('down_price', 0) or row.get('DOWN', 0))
                        
                        if up_price > 0 and up_price < 1.0:  # éªŒè¯ä»·æ ¼åˆç†æ€§
                            samples.append(PriceSample(
                                timestamp=timestamp,
                                price_cents=int(up_price * 100 + 0.5),
                                token_type='up'
                            ))
                        
                        if down_price > 0 and down_price < 1.0:  # éªŒè¯ä»·æ ¼åˆç†æ€§
                            samples.append(PriceSample(
                                timestamp=timestamp,
                                price_cents=int(down_price * 100 + 0.5),
                                token_type='down'
                            ))
                    except Exception as e:
                        continue
        except Exception as e:
            print(f"è¯»å– CSV æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return samples

class VelocityAnalyzer:
    """é€Ÿåº¦åˆ†æå™¨"""
    
    def __init__(self, window_seconds: int):
        self.window_seconds = window_seconds
        self.calculator = VelocityCalculator(window_seconds)
        self.velocity_samples = []
        self.delta_samples = []
    
    def analyze_samples(self, samples: List[PriceSample]):
        """åˆ†æä»·æ ¼æ ·æœ¬"""
        # æŒ‰æ—¶é—´æ’åº
        samples.sort(key=lambda x: x.timestamp)
        
        for sample in samples:
            self.calculator.add_sample(sample)
            
            # è®¡ç®—é€Ÿåº¦
            for token_type in ['up', 'down']:
                metrics = self.calculator.compute_velocity(token_type)
                if metrics and metrics['ok']:
                    self.velocity_samples.append(metrics['velocity'])
                    self.delta_samples.append(metrics['delta'])
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.velocity_samples:
            return {}
        
        return {
            'count': len(self.velocity_samples),
            'min_velocity': min(self.velocity_samples),
            'max_velocity': max(self.velocity_samples),
            'mean_velocity': statistics.mean(self.velocity_samples),
            'median_velocity': statistics.median(self.velocity_samples),
            'stdev_velocity': statistics.stdev(self.velocity_samples) if len(self.velocity_samples) > 1 else 0,
            'min_delta': min(self.delta_samples),
            'max_delta': max(self.delta_samples),
            'mean_delta': statistics.mean(self.delta_samples),
            'median_delta': statistics.median(self.delta_samples),
        }
    
    def calculate_probability(self, min_velocity: float, min_delta: int) -> float:
        """è®¡ç®—æ»¡è¶³æ¡ä»¶çš„æ¦‚ç‡"""
        if not self.velocity_samples:
            return 0.0
        
        count = 0
        for i, vel in enumerate(self.velocity_samples):
            delta = self.delta_samples[i]
            if vel >= min_velocity and delta >= min_delta:
                count += 1
        
        return count / len(self.velocity_samples) * 100.0

def analyze_log_file(log_file: str) -> List[PriceSample]:
    """ä»æ—¥å¿—æ–‡ä»¶æå–ä»·æ ¼æ•°æ®"""
    samples = []
    parser = LogParser()
    
    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            timestamp = parser.parse_timestamp(line)
            if not timestamp:
                continue
            
            price_info = parser.extract_price_from_log(line)
            if price_info:
                token_type, price_cents = price_info
                samples.append(PriceSample(timestamp, price_cents, token_type))
    
    return samples

def analyze_cycle_velocity(
    samples: List[PriceSample],
    window_seconds_range: List[int],
    min_velocity_range: List[float],
    min_delta_range: List[int]
) -> Dict:
    """åˆ†æå‘¨æœŸå†…çš„é€Ÿåº¦ç»Ÿè®¡"""
    results = {}
    
    for window_sec in window_seconds_range:
        analyzer = VelocityAnalyzer(window_sec)
        analyzer.analyze_samples(samples)
        
        stats = analyzer.get_statistics()
        if not stats:
            continue
        
        results[window_sec] = {
            'statistics': stats,
            'probabilities': {}
        }
        
        # è®¡ç®—ä¸åŒå‚æ•°ç»„åˆçš„æ¦‚ç‡
        for min_vel in min_velocity_range:
            for min_delta in min_delta_range:
                prob = analyzer.calculate_probability(min_vel, min_delta)
                key = f"vel_{min_vel}_delta_{min_delta}"
                results[window_sec]['probabilities'][key] = prob
    
    return results

def generate_recommendations(analysis_results: Dict, target_probability: float = 5.0) -> Dict:
    """ç”Ÿæˆå‚æ•°é…ç½®å»ºè®®"""
    recommendations = []
    
    for window_sec, data in analysis_results.items():
        stats = data['statistics']
        probs = data['probabilities']
        
        # æ‰¾åˆ°æ»¡è¶³ç›®æ ‡æ¦‚ç‡çš„å‚æ•°ç»„åˆ
        for key, prob in probs.items():
            if prob >= target_probability:
                # è§£æå‚æ•°
                parts = key.split('_')
                min_vel = float(parts[1])
                min_delta = int(parts[3])
                
                recommendations.append({
                    'windowSeconds': window_sec,
                    'minVelocityCentsPerSec': min_vel,
                    'minMoveCents': min_delta,
                    'probability': prob,
                    'expected_triggers_per_cycle': prob / 100.0 * 900  # å‡è®¾å‘¨æœŸ15åˆ†é’Ÿ=900ç§’
                })
    
    # æŒ‰æ¦‚ç‡æ’åº
    recommendations.sort(key=lambda x: x['probability'], reverse=True)
    
    return recommendations

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='VelocityHedgeHold velocity statistics analysis')
    parser.add_argument('--log-dir', type=str, default='logs', help='Log directory path')
    parser.add_argument('--data-dir', type=str, default='data', help='Data directory path (CSV files)')
    parser.add_argument('--window-seconds', type=int, nargs='+', default=[3, 5, 8, 10], help='Window size range in seconds, e.g., 3 5 8 10')
    parser.add_argument('--min-velocity', type=float, nargs='+', default=[0.2, 0.3, 0.4, 0.5, 0.6], help='Min velocity range (c/s), e.g., 0.2 0.3 0.4')
    parser.add_argument('--min-delta', type=int, nargs='+', default=[3, 4, 5, 6, 7], help='Min delta range (cents), e.g., 3 4 5 6')
    parser.add_argument('--target-probability', type=float, default=5.0, help='Target trigger probability (percent), default: 5.0')
    parser.add_argument('--output', type=str, default='velocity_analysis_report.json', help='Output JSON file path')
    
    args = parser.parse_args()
    
    print("ğŸ” å¼€å§‹åˆ†æé€Ÿåº¦ç»Ÿè®¡...")
    
    # æ”¶é›†ä»·æ ¼æ ·æœ¬
    all_samples = []
    
    # ä»æ—¥å¿—æ–‡ä»¶æå–
    log_dir = Path(args.log_dir)
    if log_dir.exists():
        print(f"ğŸ“‚ æ‰«ææ—¥å¿—ç›®å½•: {log_dir}")
        for log_file in log_dir.glob("*.log"):
            print(f"  å¤„ç†æ—¥å¿—æ–‡ä»¶: {log_file}")
            samples = analyze_log_file(str(log_file))
            all_samples.extend(samples)
            print(f"    æå–åˆ° {len(samples)} ä¸ªä»·æ ¼æ ·æœ¬")
    
    # ä» CSV æ–‡ä»¶æå–
    data_dir = Path(args.data_dir)
    if data_dir.exists():
        print(f"ğŸ“‚ æ‰«ææ•°æ®ç›®å½•: {data_dir}")
        csv_parser = CSVDataParser()
        for csv_file in data_dir.rglob("*.csv"):
            print(f"  å¤„ç† CSV æ–‡ä»¶: {csv_file}")
            samples = csv_parser.parse_csv(str(csv_file))
            all_samples.extend(samples)
            print(f"    æå–åˆ° {len(samples)} ä¸ªä»·æ ¼æ ·æœ¬")
    
    if not all_samples:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä»·æ ¼æ•°æ®ï¼")
        print("   è¯·ç¡®ä¿æ—¥å¿—ç›®å½•æˆ–æ•°æ®ç›®å½•ä¸­æœ‰æ•°æ®æ–‡ä»¶")
        sys.exit(1)
    
    print(f"\nâœ… æ€»å…±æ”¶é›†åˆ° {len(all_samples)} ä¸ªä»·æ ¼æ ·æœ¬")
    
    # æŒ‰å‘¨æœŸåˆ†ç»„åˆ†æ
    print("\nğŸ“Š å¼€å§‹ç»Ÿè®¡åˆ†æ...")
    
    # åˆ†ææ‰€æœ‰æ ·æœ¬
    analysis_results = analyze_cycle_velocity(
        all_samples,
        args.window_seconds,
        args.min_velocity,
        args.min_delta
    )
    
    # ç”Ÿæˆå»ºè®®
    recommendations = generate_recommendations(analysis_results, args.target_probability)
    
    # è¾“å‡ºç»“æœ
    output_data = {
        'summary': {
            'total_samples': len(all_samples),
            'analysis_windows': list(analysis_results.keys()),
            'total_recommendations': len(recommendations)
        },
        'analysis_results': analysis_results,
        'recommendations': recommendations[:20]  # åªæ˜¾ç¤ºå‰20ä¸ª
    }
    
    # ä¿å­˜ JSON
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*80)
    print("ğŸ“ˆ é€Ÿåº¦ç»Ÿè®¡æ‘˜è¦")
    print("="*80)
    
    for window_sec, data in analysis_results.items():
        stats = data['statistics']
        print(f"\nçª—å£å¤§å°: {window_sec}ç§’")
        print(f"  æ ·æœ¬æ•°: {stats['count']}")
        print(f"  é€Ÿåº¦èŒƒå›´: {stats['min_velocity']:.3f} - {stats['max_velocity']:.3f} (c/s)")
        print(f"  å¹³å‡é€Ÿåº¦: {stats['mean_velocity']:.3f} (c/s)")
        print(f"  ä¸­ä½æ•°é€Ÿåº¦: {stats['median_velocity']:.3f} (c/s)")
        print(f"  ä½ç§»èŒƒå›´: {stats['min_delta']} - {stats['max_delta']} (c)")
        print(f"  å¹³å‡ä½ç§»: {stats['mean_delta']:.1f} (c)")
    
    print("\n" + "="*80)
    print("ğŸ’¡ å‚æ•°é…ç½®å»ºè®®ï¼ˆæŒ‰è§¦å‘æ¦‚ç‡æ’åºï¼‰")
    print("="*80)
    
    for i, rec in enumerate(recommendations[:10], 1):
        print(f"\nå»ºè®® #{i}:")
        print(f"  windowSeconds: {rec['windowSeconds']}")
        print(f"  minVelocityCentsPerSec: {rec['minVelocityCentsPerSec']}")
        print(f"  minMoveCents: {rec['minMoveCents']}")
        print(f"  é¢„æœŸè§¦å‘æ¦‚ç‡: {rec['probability']:.2f}%")
        print(f"  é¢„æœŸæ¯å‘¨æœŸè§¦å‘æ¬¡æ•°: {rec['expected_triggers_per_cycle']:.1f}")

if __name__ == '__main__':
    main()
